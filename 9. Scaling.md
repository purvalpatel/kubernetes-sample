Autoscaling:
-------------
Up or Down the resources according to usage.

Think of it like:<br><br>

When your website gets more visitors â†’ Kubernetes automatically adds more servers (Pods).<br>
When traffic goes down â†’ It removes extra ones to save cost.<br>

Methods: <br>
| Type                                | What it Scales          | Example Use                         |
| ----------------------------------- | ----------------------- | ----------------------------------- |
| **HPA** â€“ Horizontal Pod Autoscaler | Number of Pods          | More traffic â†’ more Pods            |
| **VPA** â€“ Vertical Pod Autoscaler   | CPU/memory for each Pod | Same Pods â†’ give them more power    |
| **CA** â€“ Cluster Autoscaler         | Number of Nodes         | Adds/removes Nodes from the cluster |


Two scale commands are there,
1. Scale
2. autoscale ( it uses HPA by default )
   
### ðŸ§© 1. Horizontal Pod Autoscaler (HPA)
- **Increases or decreases Pod replicas automatically based on CPU or memory usage.**
HPA by default comes with the kubernetes.

You already have a Deployment:<br>
```
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80
```

Now enable autoscaling<br>
```
kubectl autoscale deployment nginx --min=2 --max=10 --cpu-percent=70
```


Below is the YAML of horizontal pod autoscaler.

```YAML
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

- Minimum 2 Pods
- Maximum 10 Pods
- If CPU usage goes above 70%, add more Pods
- If CPU usage drops below, remove Pods

Check status:
```
kubectl get hpa
```


### ðŸ§© 2. Vertical Pod Autoscaler (VPA)
While HPA scales the number of pods <br>
VPA Adjusts CPU and memory requests/limits for Pods automatically.<br>
VPA doesn't come with kubernetes by default, it is seperate project you can get from github.<br>

#### Install VerticalPodAutoscaler:<br>
```
git clone https://github.com/kubernetes/autoscaler.git
cd vertical-pod-autoscaler
./hack/vpa-up.sh
```
Now it is installed, you can verify with:
```
kubectl describe vpa
kubectl --namespace=kube-system get pods|grep vpa
kubectl get customresourcedefinition | grep verticalpodautoscalers
```

Beelow is the example of VPA configuration:
```YAML
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: nginx
  updatePolicy:
    updateMode: "Auto"  # Can be "Off", "Initial", or "Auto"
```
Apply it:
```
kubectl apply -f vpa.yaml
```
#### Limitations:
Whenever VPA updates the pod resources, the pod is recreated, which causes all running containers to be recreated. The pod may be recreated on a different node.

### ðŸ§© 3. Cluster Autoscaler (CA)
if Pods canâ€™t start because there arenâ€™t enough Nodes, Cluster Autoscaler adds new Nodes (VMs). <br>
When load decreases, it removes unused Nodes. <br>

This usually works in cloud environments like AWS, GCP, Azure, etc.<br>


KEDA:
------

The Default kubernetes metrices for scaling is CPU, RAM. <br>
if we wanted to go above this then we have to use Advance kubernetes functionality use on top of this. i.e. **KEDA**  **(Kubernetes Event driven Autoscaling)** <br>

KEDA Work along side with the existing kubernetes component like HPA and can extend functionality without duplication or overwriding. <br>

**Deployment document**: https://keda.sh/docs/2.18/deploy/

**Install KEDA with Helm.<br>**

Now, we will understand **how we use it**. <br>

We, have grafana and prometheus setup. in Grafana there is metrics called, Latency is showing.<br>

**What is latency(95% percentile)?<br>**
95th percentile latency is a statistical metric used to measure the performance of a system, particulary in latency-sensitive applications like web service, APIs, or databases.<br>

Means, <br>
95% requests are faster<br>
5% Requests are slower.<br>

Now we will take this data from** prometheus for scaling**.<br>

For that KEDA gives us configuration called **scaledObject**.<br>
If you are directly dealing with kubernetes then you would use **hpa**.<br>
in this case we are using **scaledobject**, which is a wrapper on top of that.<br>

Below is the example of http requests.
```YAML
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
   name: http-app-scaledobject
spec:
   scaleTargetRef:
      name: http-app
   minReplicaCount: 1
   maxReplicaCount: 10
   triggers:
      - type: prometheus
        metadata:
           serverAddress: http://prometheus-server.default.svc.cluster.local:9090
           metricName: http_requests_total
           threshold: '5'
           query: sum(rate(http_requests_total[1m])) 
```
Check created scaledobject.
```
kubectl get scaledobject
```
Check created hpa:<br>
KEDA will create linked Autoscaler automatically with scaledobject.
```
kubectl get hpa
```
You can add **multiple triggers in scaledobject.**<br>
Just add one more block.<br>

### Now we will do Load Test:

Install some tool like, HTTP tool generator, **hey**<br>
```
hey -z 1m -c 10 http://<EXTERNAL-IP>
```





